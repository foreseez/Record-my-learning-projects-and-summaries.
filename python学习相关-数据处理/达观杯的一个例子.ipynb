{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'classify'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32mD:\\Anaconda\\lib\\site-packages\\pandas\\core\\indexes\\base.py\u001b[0m in \u001b[0;36mget_loc\u001b[1;34m(self, key, method, tolerance)\u001b[0m\n\u001b[0;32m   3062\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3063\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3064\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'classify'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-ff8006007cf3>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[0mtest_term_doc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mvec\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mcolumn\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 16\u001b[1;33m \u001b[0my\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"classify\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mint\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     17\u001b[0m \u001b[0mclf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mLogisticRegression\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mC\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m4\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdual\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[0mclf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrn_term_doc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda\\lib\\site-packages\\pandas\\core\\frame.py\u001b[0m in \u001b[0;36m__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   2683\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_getitem_multilevel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2684\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2685\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_getitem_column\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2686\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2687\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_getitem_column\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda\\lib\\site-packages\\pandas\\core\\frame.py\u001b[0m in \u001b[0;36m_getitem_column\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   2690\u001b[0m         \u001b[1;31m# get column\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2691\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_unique\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2692\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_item_cache\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2693\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2694\u001b[0m         \u001b[1;31m# duplicate columns & possible reduce dimensionality\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda\\lib\\site-packages\\pandas\\core\\generic.py\u001b[0m in \u001b[0;36m_get_item_cache\u001b[1;34m(self, item)\u001b[0m\n\u001b[0;32m   2484\u001b[0m         \u001b[0mres\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcache\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2485\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mres\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2486\u001b[1;33m             \u001b[0mvalues\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_data\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2487\u001b[0m             \u001b[0mres\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_box_item_values\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalues\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2488\u001b[0m             \u001b[0mcache\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mres\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda\\lib\\site-packages\\pandas\\core\\internals.py\u001b[0m in \u001b[0;36mget\u001b[1;34m(self, item, fastpath)\u001b[0m\n\u001b[0;32m   4113\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4114\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0misna\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 4115\u001b[1;33m                 \u001b[0mloc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   4116\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4117\u001b[0m                 \u001b[0mindexer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0misna\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda\\lib\\site-packages\\pandas\\core\\indexes\\base.py\u001b[0m in \u001b[0;36mget_loc\u001b[1;34m(self, key, method, tolerance)\u001b[0m\n\u001b[0;32m   3063\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3064\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3065\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_maybe_cast_indexer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3066\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3067\u001b[0m         \u001b[0mindexer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_indexer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmethod\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtolerance\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtolerance\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'classify'"
     ]
    }
   ],
   "source": [
    "import pandas as pd, numpy as np\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "import time\n",
    "t1=time.time()\n",
    "train = pd.read_csv('./daguan_data/train_set.csv')\n",
    "test = pd.read_csv('./daguan_data/test_set.csv')\n",
    "test_id = pd.read_csv('./daguan_data/test_set.csv')[[\"id\"]].copy()\n",
    "\n",
    "column=\"word_seg\"\n",
    "n = train.shape[0]\n",
    "vec = TfidfVectorizer(ngram_range=(1,2),min_df=3, max_df=0.9,use_idf=1,smooth_idf=1, sublinear_tf=1)\n",
    "trn_term_doc = vec.fit_transform(train[column])\n",
    "test_term_doc = vec.transform(test[column])\n",
    "\n",
    "y=(train[\"classify\"]-1).astype(int)\n",
    "clf = LogisticRegression(C=4, dual=True)\n",
    "clf.fit(trn_term_doc, y)\n",
    "preds=clf.predict_proba(test_term_doc)\n",
    "\n",
    "#保存概率文件\n",
    "test_prob=pd.DataFrame(preds)\n",
    "test_prob.columns=[\"class_prob_%s\"%i for i in range(1,preds.shape[1]+1)]\n",
    "test_prob[\"id\"]=list(test_id[\"id\"])\n",
    "test_prob.to_csv('./daguan_data/prob_lr_baseline.csv',index=None)\n",
    "\n",
    "#生成提交结果\n",
    "preds=np.argmax(preds,axis=1)\n",
    "test_pred=pd.DataFrame(preds)\n",
    "test_pred.columns=[\"class\"]\n",
    "test_pred[\"class\"]=(test_pred[\"class\"]+1).astype(int)\n",
    "print(test_pred.shape)\n",
    "print(test_id.shape)\n",
    "test_pred[\"id\"]=list(test_id[\"id\"])\n",
    "test_pred[[\"id\",\"class\"]].to_csv('sub_lr_baseline.csv',index=None)\n",
    "t2=time.time()\n",
    "print(\"time use:\",t2-t1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1、比赛任务描述\n",
    "这次比赛的任务本质上就是一个长文本分类任务，目标是“建立模型通过长文本数据正文，预测文本对应的类别”，比赛的分类类别一共有19种。例如给定一篇新闻文章（News Articles），预测这篇文章的类别是Food，Sports还是Politics。\n",
    "\n",
    "\n",
    "1.1 评分标准\n",
    "采用各个类别F1指标的算术平均值。其中，Pi表示第i个类别对应的准确率，Ri表示第i个类别对应的召回率。\n",
    "\n",
    "\n",
    "2、数据介绍\n",
    "数据包含2个csv文件，train_set.csv和test_set.csv，分别用于训练模型和测试。值得注意的有以下几点：\n",
    "\n",
    "数据每一行对应一篇文章，文章分别在“字”和“词”的级别上做了脱敏处理，也就是说数据中我们看不到具体的字和词，只有它们的数字id，每个id对应一个字或词。\n",
    "一篇文章的文本数据包含两类，分别是文章正文在“字”级别上的表示article，和在“词”级别上的表示word。\n",
    "2.1 数据分析\n",
    "经过统计，train_set.csv和test_set.csv分别包含十万条数据，包含1万多个不同article和84万多个不同word。数据的平均字长度为1177，平均词长度为718，可以看出文本非常长，这也是本次比赛数据集最关键的特点。\n",
    "\n",
    "2.2 数据预处理\n",
    "将train_set.csv按照9:1的比例划分为训练集和验证集，应用深度学习模型需要将文本进行截断补齐，这里的截断补齐长度分别为2000和3200，采用的策略是要求这个长度能够覆盖95%的数据。\n",
    "\n",
    "\n",
    "文本预处理部分用到了torchtext，torchtext是专门用于文本预处理的python包，使用简单，可以节省很多文本预处理部分的工作，具体的使用参考比赛源码和官方文档pytorch/text。\n",
    "\n",
    "2.3 预训练字/词向量\n",
    "因为这次的文本数据经过脱敏，因此无法利用外部的预训练字/词向量，需要我们自己训练。我们使用word2vec包：danielfrg/word2vec进行预训练，向量维度为300维。（赛后听其他组的分享，分别用word2vec，glove，fasttext训练100维向量，然后拼接成300维作为字/词向量，能获得更好的模型训练结果）\n",
    "\n",
    "3、模型介绍\n",
    "基于深度学习的文本分类中总结：\n",
    "\n",
    "深度学习文本分类模型概括来说，主要有五大类模型：\n",
    "1、词嵌入向量化：word2vec，FastText等\n",
    "2、卷积神经网络特征提取：Text-CNN，Char-CNN等\n",
    "3、上下文机制：Text-RNN，BiRNN，RCNN等\n",
    "4、记忆存储机制：EntNet，DMN等\n",
    "5、注意力机制：HAN等\n",
    "其中我们采用的模型有五种：\n",
    "\n",
    "TextCNN\n",
    "GRU\n",
    "RCNN\n",
    "FastText\n",
    "Attention-GRU\n",
    "模型的具体介绍建议仔细阅读知乎专栏文章用深度学习（CNN RNN Attention）解决大规模文本分类问题 - 综述和实践和基于深度学习的文本分类，这里我就不一一介绍了，值得注意的是这些模型的总体架构都是类似的：都是先将文本送入embedding层，然后通过CNN/GRU/RCNN等模型提取特征，最终通过分类器输出19维向量，分别对应19个类别的分类概率。\n",
    "\n",
    "\n",
    "3.1 模型实现\n",
    "深度学习模型实现我们采用了PyTorch，实现代码见比赛源码moneyDboat/data_grand中models文件夹，作为Pytorch的忠实拥趸，我之后会专门写一篇文章详细介绍说明，Pytorch相较于其他深度学习框架例如tensorflow、caffe等的优势，简单来说有这么几点：\n",
    "\n",
    "简洁易用：Pytorch的设计追求最少的封装，尽量避免重复造轮子。\n",
    "动态图实现：代码简洁直观，同时让调试更加简单。\n",
    "易于学习掌握：Pytorch提供了清晰的文档和指南，以及论坛PyTorch Forums，遇到的问题基本上都能在论坛中查到。\n",
    "\n",
    "3.2 传统机器学习模型\n",
    "因为这次的文本非常长，所以传统的机器学习模型也能取得非常不错的效果，tfidf+svm的baseline在测试集上的f1就能达到0.779。\n",
    "\n",
    "3.3 训练策略\n",
    "训练策略很关键，参考知乎“看山杯” 夺冠记：\n",
    "\n",
    "1、刚开始训练的时候Embedding层的学习率为0，其它层的学习率为1e-3，采用Adam优化器（一开始的时候卷积层都是随机初始化的，反向传播得到的Embedding层的梯度受到卷积层的影响，相当于噪声）\n",
    "2、训练1-2个epoch之后，Embedding层的学习率设为2e-4\n",
    "3、每个epoch或者半个epoch统计一次在验证集的分数：\n",
    "如果分数上升，保存模型，并记下保存路径；如果分数下降，加载上一个模型的保存路径，并降低学习率为一半（重新初始化优化器，清空动量信息，而不是只修改学习率----使用PyTorch的话新建一个新优化器即可）\n",
    "预训练字/词向量在模型训练过程中的调整非常必要，因为word2vec等其他预训练方法都是通过无监督训练得到的，没有利用到训练集的分类类别信息。实验表明，embedding层设置学习率之后，模型的f1值大约能提升3个百分点。\n",
    "\n",
    "3.4 单模型结果\n",
    "时间所限，另外一个模型训练完需要超过十个小时，我们没有进行特别细致的调参，最后得到的单模型结果如下：\n",
    "\n",
    "\n",
    "4、模型融合\n",
    "模型融合部分我们尝试过多种方法，最终还是采用过了最简单但效果非常稳定的概率等权重融合。将上述深度学习模型以及传统机器学习模型融合后线上得分能达到0.7970。\n",
    "\n",
    "\n",
    "做到这里，我们陷入了瓶颈，再怎么用原来的模型做融合也没办法提高线上的分数，寻找一些其他有效的突破口。关于模型融合，周志华教授的西瓜书中提到：\n",
    "\n",
    "模型融合中个体学习器需要“好而不同”。\n",
    "就是说模型之间差异性越大，融合效果越好。那么如何创造差异性，有两种方法：\n",
    "\n",
    "尝试不同模型\n",
    "改变模型输入\n",
    "其中第一种方法“尝试不同模型”，因为当时我们距离比赛结束只剩下不到三天的时间，再去实现新的模型不太现实，所以我们考虑采用第二种方法：改变模型输入。经过尝试，我们发现两种有效的方法：\n",
    "\n",
    "重新划分训练集、验证集，用新的训练集训练数据\n",
    "将文本倒序之后训练新的模型\n",
    "两种方法都是考虑到原本的验证集和文本预处理中被截断的部分，损失了训练集的部分信息，通过这两种方式训练新的模型用于模型融合，最终的线上分数能提升近3个千分点，达到0.79995，让我们得以进入Top10。\n",
    "\n",
    "\n",
    "5、总结\n",
    "先谈一下自己的体会，这次是我第一次参加算法类的比赛，前后总共花了3个星期左右的时间完成比赛，这三个星期中确实非常辛苦，当然最辛苦的大概是服务器哈哈，从开始到比赛结束都没有休息过，花的电费应该也不少。自己基本上都是白天写代码，晚上回宿舍前在服务器上跑模型，第二天早上第一件事就是查看模型训练结果，看看一晚上的收成怎么样，然后继续改代码调参数，就这样不断循环往复，好在最后的比赛结果还算满意，自己也收获了很多经验。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
